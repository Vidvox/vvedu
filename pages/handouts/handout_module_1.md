---
title: Live Visuals
keywords: 
last_updated: August 22, 2018
tags: [handouts]
summary: "Handout for Live Visuals"
sidebar: home_sidebar
permalink: handout_module_1.html
folder: handouts
---

# “Live” Visuals

VJ’ing stems from “[Projection Design](https://www.svenortel.com/projection-design-as-design-discipline/),” a term chosen by scenic artists to refer to designers who worked with film and video projectors in theatre. The title has since grown beyond the technical. It now includes LED fixtures, TV monitors, computer screens, interactive visual instruments, Pepper's Ghosts and Eyeliners to creative holographic images, and so on. Our field now generally involves putting an image on the stage that is ephemeral and changeable.

The input and output of your images will have a great influence on content, context, aesthetics, and meaning. It is also a great way to play with “practical” effects, by having the materiality of the “screen” and/or the inherent aesthetics of the input device help further define your art. 

# Lesson 1: Single and Multi-Channel Input / Output

Multi-channel output can add a physical, spatial element to your images, which is a great way to present split screen content, creating an immersive, spatial presentation of imagery. In the event of custom screens or architectural projection, a VJ can use mapping tools to crop and transform the imagery to fit seamlessly into the environment.

In addition to multiple outputs, a VJ might incorporate live-camera feeds into their performance. This adds an additional element of presence for both the performer and the audience, allowing a more intimate window into the processes onstage, making the performer a character in the visual narrative. 

Multiple cameras can present different views of the performer at the same time, also called multiple perspective, [simultaneity](https://en.wikipedia.org/wiki/Simultaneity) or multiplicity. This allows an otherwise impossible, holistic viewing experience for every member of the audience, regardless of where they stand.

This week, we will experiment with the addition of live-camera feeds into your project. Consider how the live presence of your face/body influences the meaning of your imagery. You are now part of the subject matter, so consider your relationship to the pre-existing forms. 

We will then explore multiple outputs from VDMX, and how the context and relationship of the imagery changes when seen from multiple perspectives. 

## Lesson Overview

Inputs
* Cameras
* Media files
* Capture devices (blackmagic)
* NDI
* Syphon
* Screen / window capture
* Internal feedback loops
FX
* Adding filters between input and output
* Cropping
Outputs
* Fullscreen output
* Recording movies
* Perspective correction
* Using projectors / displays
* System Preferences Displays
* Color calibration

## Reference Links

* [The AVF Batch Exporter](https://docs.vidvox.net/freebies_avf_batch_exporter.html)
* [Survey of Alternative Displays](http://blairneal.com/survey-of-alternative-displays/)
* [Guide to Projectors for Interactive Installations](https://github.com/laserpilot/Guide_To_Projectors_For_Interactive_Installations/blob/master/Guide%20to%20Projectors%20for%20Interactive%20Installations.md)
* [Guide to Cameras for Interactive Installations](https://github.com/laserpilot/Guide_To_Cameras_Interactive_Installations/blob/master/Guide_To_Cameras_For_Interactive_Installations.md)

## Related Tutorials

* [Using the Simple Player template](https://vdmx.vidvox.net/tutorials/using-the-simple-player-template)
* [Recording movies in VDMX](https://vdmx.vidvox.net/tutorials/recording-movies-to-disk)
* [Recording movies from cameras in VDMX](https://vdmx.vidvox.net/tutorials/multi-channel-live-camera-video-sampler)
* [Fullscreen output](https://vdmx.vidvox.net/tutorials/outputing-in-fullscreen-mode)
* [Video Input Basics](https://vdmx.vidvox.net/tutorials/video-inputs-basics)
* [Multi-channel Live Camera Video Sampler](https://vdmx.vidvox.net/tutorials/multi-channel-live-camera-video-sampler)
* [Multi-screen output from VDMX](https://vdmx.vidvox.net/tutorials/multi-display-video-mixing-with-vdmx-on-a-retina-macbook-pro)
* [Creating video feedback loops in VDMX](https://vdmx.vidvox.net/tutorials/creating-video-feedback-loops-on-a-mac-with-vdmx)
* [Converting media files to HAP](http://hap.video/using-hap.html)

## Case Studies

* [Using Lumen and VDMX together with Syphon with guest Wiley Wiggins](https://vdmx.vidvox.net/tutorials/wiley-wiggins-lumen-and-vdmx)
* [DMX Controlled Shobaleader One Face Masks](https://vdmx.vidvox.net/blog/shobaleader-one-masks)

## Homework

* Record 5 clips using a live web-cam with different sets of FX applied.
* Connect to an external display (such as a projector, TV or monitor) using an HDMI, DVI, VGA or similar connection.

# Lesson 2: Responsiveness

VJ’ing design is a [cybernetic](https://en.wikipedia.org/wiki/Cybernetics) art form; we are essentially creating a visual instrument. The goal of many visual performers is to have a close and immediate interface with computers, to make them expressive. Their goal is to use these machines to emote, thereby making the computer's presence invisible. 

For live performance, particularly for live music, the element of improvisation and responsiveness matches the energy and ephemeral quality of the performance in a way that pre-rendered and cued/time coded imagery cannot. 

In addition, the imagery and its delivery systems (playback software, MIDI controllers, analog mixers, and so on) can be refined and tweaked over time, similar to way music may evolve during rehearsals on a tour—fusing a symbiotic relationship between the musicians and the visualist.

The presence of real-time effects and audio-responsive imagery increases the synaesthetic relationship between image and sound, thereby creating a more “live” experience for the audience.

This week, we will explore interactive concepts that extend the moving image beyond the timeline to real-time interactive expression, using data mappings from physical interfaces such as keyboards, MIDI, OSC and DMX lighting boards.

## Lesson Overview

* Linking layer and FX controls to Audio Analysis 
* MIDI and OSC instruments / controllers

* Add audio reactivity to FX and layer parameters
* Add MIDI (or OSC) control to FX and layer parameters

## Reference Links

* [Soundflower](https://github.com/mattingalls/Soundflower/releases)
* MIDI Controllers discussion thread on forums

## Related Tutorials

* [4 Layer Korg nanoKontrol Template](https://vdmx.vidvox.net/tutorials/4-layer-korg-nanokontrol2-template)
* [Enabling MIDI and OSC Echo Mode](https://vdmx.vidvox.net/tutorials/using-echo-mode-for-easy-setup-midi-and-osc-talkback-in-vdmx)
[Setting up MIDI Bin sync with the APC20](https://vdmx.vidvox.net/tutorials/setting-up-media-bin-ui-sync-with-the-apc20-apc40)
* [Automatic BPM Detection in VDMX](https://vdmx.vidvox.net/tutorials/how-to-build-the-waveclock-example-template)
* [Using game controllers in VDMX](https://vdmx.vidvox.net/tutorials/using-video-game-controllers-with-vdmx)

## Case Studies

* [The ECLECTIC METHOD REMIX, Part Two - Jamming](https://vdmx.vidvox.net/tutorials/the-eclectic-method-remix-part-two-jamming)
* [PZYK SKAN – EEG Controlled Sound and Visuals](https://vdmx.vidvox.net/blog/pzyk-skan-eeg-controlled-visuals)

## Homework

* Record 5 clips, using any single source type (eg live web-cam), with different sets of FX applied while using audio analysis, MIDI or OSC to control the parameters.