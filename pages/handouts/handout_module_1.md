---
title: Live Visuals
keywords: 
last_updated: August 22, 2018
tags: [handouts]
summary: "Handout for Live Visuals"
sidebar: home_sidebar
permalink: handout_module_1.html
folder: handouts
---

# “Live” Visuals

VJ’ing stems from “[Projection Design](https://www.svenortel.com/projection-design-as-design-discipline/),” a term chosen by scenic artists to refer to designers who worked with film and video projectors in theatre. The title has since grown beyond the technical. It now includes LED fixtures, TV monitors, computer screens, interactive visual instruments, Pepper's Ghosts and Eyeliners to creative holographic images, and so on. Our field now generally involves putting an image on the stage that is ephemeral and changeable.

The input and output of your images will have a great influence on content, context, aesthetics, and meaning. It is also a great way to play with “practical” effects, by having the materiality of the “screen” and/or the inherent aesthetics of the input device help further define your art. 

## Lesson 1: Input To Output

In this lesson we will explore the three most basic parts of our VJ toolkit – inputs, filters and outputs.

The inputs, or sources, are the materials that contain the content of our work, whether they be pre-rendered files, live feeds or real-time computer generated visuals.

This imagery is then processed by “filters” also commonly known as effects or FX, which modify the stream of pictures to change the aesthetics and overall feeling associated with them.

Finally the result is sent to a place where it can be viewed, output to a screen, projector, a movie file on a hard drive or streamed to the Internet.

One of the most powerful aspects of working with “live” video is the ability to experiment with the addition of live-camera feeds into your project. Consider how the live presence of your face/body influences the meaning of your imagery. You are now part of the subject matter, so consider your relationship to the pre-existing forms.

### Videos and Slides

* [Input to Output Lecture Video](https://vimeo.com/289316023)
* [Input to Output Lecture Slides](https://docs.google.com/presentation/d/12Arx326oMkRXVu9xjnftlviyvkbhXMy1qJ31yW3yVuc)
* [Input to Output Demonstration Video](https://vimeo.com/289319159)

### Lesson Overview

What are visuals? A historical context.
Inputs / Sources
* Pre-rendered files
* Interactive generators
* Live feeds
FX and Composition
* Adding filters between input and output
* Adding layers
* Adjusting layer opacity and blend modes
Outputs
* Fullscreen output
* Recording movies
* * Capture to h.264 for uploads to YouTube, Vimeo
* * Capture to PhotoJPEG / HAP for live remixing
* Perspective correction
* Using projectors / displays
* System Preferences: Displays
* Color calibration

### Lecture Notes

- What are visuals? A historical context.
- - Theater
- - Light projections
- - Film
- - Video
- What is video?
- - What is a video signal?
- - - How is video different from film?
- - - Analog (eg NTSC, PAL) vs Digital
- - How is video transmitted and stored?
- - - Types of cables
- - - Physical media and digital files
- What is a media codec?
- - How do image / video codecs work?
- - What image / video codecs are commonly used today?
- What is video videoinstrumentalism?

### Reference Links

* [Nam June Paik - Videofilm Concert](https://www.youtube.com/watch?v=VEAUjFLSqXY)
* [Coldcut – Timber](https://www.youtube.com/watch?v=5-wl7Xk5FoY)
* [Emergency Broadcast Network – We Will Rock You ](https://www.youtube.com/watch?v=2OfFloEMhcg)

* [Recommended video codecs for VDMX](https://docs.vidvox.net/vdmx_video_codecs.html)
* [Converting media files to HAP](http://hap.video/using-hap.html)
* [Video codec usage comparisons and benchmarks](http://hap.video/benchmarks.html)
* [Guide to Projectors for Interactive Installations](https://github.com/laserpilot/Guide_To_Projectors_For_Interactive_Installations/blob/master/Guide%20to%20Projectors%20for%20Interactive%20Installations.md)
* [Guide to Cameras for Interactive Installations](https://github.com/laserpilot/Guide_To_Cameras_Interactive_Installations/blob/master/Guide_To_Cameras_For_Interactive_Installations.md)
* [YouTube.com](https://www.youtube.com/)
* [Vimeo.com](https://www.vimeo.com/)
* [What's In A GIF?](http://giflib.sourceforge.net/whatsinagif/bits_and_bytes.html)
* [How does analog video work?](https://www.slideshare.net/DSPIP/analog-video)

### Resources

* [Sample Media files](https://s3.amazonaws.com/vidvox/vvedu/M1-L1.zip)
* [The AVF Batch Exporter](https://docs.vidvox.net/freebies_avf_batch_exporter.html)
* [VDMX Sample Media files](https://docs.vidvox.net/vdmx_sample_media.html)
* [Project Milk Syphon](https://docs.vidvox.net/freebies_project_milk_syphon.html)

### Related Tutorials and Case Studies

* [Video Fundamentals: Media Types](https://vdmx.vidvox.net/tutorials/video-fundamentals-taught-with-vdmx-part-2)
* [Using the Simple Player template](https://vdmx.vidvox.net/tutorials/using-the-simple-player-template)
* [Video Input Basics](https://vdmx.vidvox.net/tutorials/video-inputs-basics)
* [Recording movies in VDMX](https://vdmx.vidvox.net/tutorials/recording-movies-to-disk)
* [Recording movies from cameras in VDMX](https://vdmx.vidvox.net/tutorials/multi-channel-live-camera-video-sampler)
* [Fullscreen output](https://vdmx.vidvox.net/tutorials/outputing-in-fullscreen-mode)
* [Recording movies for upload](https://vdmx.vidvox.net/tutorials/recording-a-demo-reel-to-share-online)
* [Using an iOS Device as a Live Camera Source in VDMX](https://vdmx.vidvox.net/tutorials/ios-device-as-live-input-for-vdmx)
* [Creating video feedback loops in VDMX](https://vdmx.vidvox.net/tutorials/creating-video-feedback-loops-on-a-mac-with-vdmx)
* [Using Lumen and VDMX together with Syphon with guest Wiley Wiggins](https://vdmx.vidvox.net/tutorials/wiley-wiggins-lumen-and-vdmx)
* [DMX Controlled Shobaleader One Face Masks](https://vdmx.vidvox.net/blog/shobaleader-one-masks)

### Homework

* Record 5 clips using a live web-cam with different sets of FX applied.
* Connect to an external display (such as a projector, TV or monitor) using an HDMI, DVI, VGA or similar connection.

## Lesson 2: Responsiveness

VJ’ing design is a [cybernetic](https://en.wikipedia.org/wiki/Cybernetics) art form; we are essentially creating a visual instrument. The goal of many visual performers is to have a close and immediate interface with computers, to make them expressive. Their goal is to use these machines to emote, thereby making the computer's presence invisible. 

For live performance, particularly for live music, the element of improvisation and responsiveness matches the energy and ephemeral quality of the performance in a way that pre-rendered and cued/time coded imagery cannot. 

In addition, the imagery and its delivery systems (playback software, MIDI controllers, analog mixers, and so on) can be refined and tweaked over time, similar to way music may evolve during rehearsals on a tour—fusing a symbiotic relationship between the musicians and the visualist.

The presence of real-time effects and audio-responsive imagery increases the synaesthetic relationship between image and sound, thereby creating a more “live” experience for the audience.

This week, we will explore interactive concepts that extend the moving image beyond the timeline to real-time interactive expression, using data mappings from physical interfaces such as keyboards, MIDI, OSC and DMX lighting boards.

### Videos and Slides

* [Responsiveness Lecture Video](https://vimeo.com/290539286)
* [Responsiveness Lecture Slides](https://docs.google.com/presentation/d/1Dd04BLD53DW9HhfK7iyAmnumrnaLyUA8HimJAjkzSrY)
* [Responsiveness Demonstration Video](https://vimeo.com/290573189)

### Lesson Overview

* What is responsiveness?
* What is a cybernetic artform?
* What are physical and sensory inputs?
* What are MIDI, DMX and OSC? In what ways are they different?

* Using MIDI and OSC instruments / controllers
* Using the Control Surface plugin to create a virtual instrument

* Add audio reactivity to FX and layer parameters
* Add MIDI control to FX and layer parameters
* Add OSC control to FX and layer parameters
* Media bin setup
* * Keyboard / MIDI / OSC triggers
* * Enabling media preloading
* Control surface plugin
* * Adding UI items
* * Custom layouts
* * Control from webpage

### Lecture Notes

- What is responsiveness?
- What is a cybernetic artform?
- What physical interfaces are used to “perform” with computers / machines?
- What are MIDI, DMX and OSC? In what ways are they different?
- - What is MIDI?
- - What is DMX?
- - What is OSC?
- - A comparison of MIDI / DMX / OSC
- - Choosing a MIDI / DMX / OSC controller
- What are sensory inputs?
- - Sound
- - Visual

### Reference Links

* [Sample project](https://s3.amazonaws.com/vidvox/vvedu/M1-L2.zip)
* [Soundflower](https://github.com/mattingalls/Soundflower/releases)
* [Facetracker](http://facetracker.net/) / [FaceOSC](https://github.com/kylemcdonald/ofxFaceTracker/releases)
* [TouchOSC](https://hexler.net/software/touchosc)
* [Lemur](https://liine.net/en/products/lemur/)
* [MIDI Controllers discussion thread on forums](https://discourse.vidvox.net/t/favorite-midi-controller-thread/46/1)

### Related Tutorials and Case Studies

* [4 Layer Korg nanoKontrol Template](https://vdmx.vidvox.net/tutorials/4-layer-korg-nanokontrol2-template)
* [Enabling MIDI and OSC Echo Mode](https://vdmx.vidvox.net/tutorials/using-echo-mode-for-easy-setup-midi-and-osc-talkback-in-vdmx)
[Setting up MIDI Bin sync with the APC20](https://vdmx.vidvox.net/tutorials/setting-up-media-bin-ui-sync-with-the-apc20-apc40)
* [Using game controllers in VDMX](https://vdmx.vidvox.net/tutorials/using-video-game-controllers-with-vdmx)
* [The ECLECTIC METHOD REMIX, Part Two - Jamming](https://vdmx.vidvox.net/tutorials/the-eclectic-method-remix-part-two-jamming)
* [PZYK SKAN – EEG Controlled Sound and Visuals](https://vdmx.vidvox.net/blog/pzyk-skan-eeg-controlled-visuals)
* [FaceOSC](https://vimeo.com/26098366)
* [Making custom Face Tracking FX in Quartz Composer](https://vdmx.vidvox.net/tutorials/making-face-tracking-fx-for-vdmx-with-quartz-composer)

### Homework

* Record 5 short “gestures” as a single movie file, using any single source type (eg live web-cam), with different sets of FX applied while using audio analysis, MIDI or OSC to control the parameters. Each gesture should be no longer than 4-16 seconds in length with a short pause (“rest”) in between each section.